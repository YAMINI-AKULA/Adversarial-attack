# Adversarial-attack

## 𝐌𝐲 𝐉𝐨𝐮𝐫𝐧𝐞𝐲 𝐰𝐢𝐭𝐡 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐯𝐞 𝐀𝐝𝐯𝐞𝐫𝐬𝐚𝐫𝐢𝐚𝐥 𝐀𝐭𝐭𝐚𝐜𝐤𝐬 
[Paper 🔗](https://lnkd.in/gJTUEhbT)


💡In this project i will be exploring an generative adversarial attack on image classification model (CNN) and will evaluate the models accruacy with and without the attack.
I have use the perbuted images along with the true images and noticed a significant drop in accracy from  91% to 30%.

Understanding and mitigating adversarial attacks is crucial for building robust and reliable AI systems. This project highlights the vulnerabilities that can be exploited in machine learning models and underscores the need for developing stronger defenses.
 
🔍 𝐏𝐫𝐨𝐣𝐞𝐜𝐭 𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:
• Developed an image classification model to categorize a deck of cards using PyTorch.
• Utilized the variational autoencoder technique to generate perturbed images.
• Observed a significant drop in model accuracy from 96% to 31%, highlighting the impact of adversarial attacks.
 
🛡 𝐖𝐡𝐚𝐭 𝐢𝐬 𝐚𝐧 𝐀𝐝𝐯𝐞𝐫𝐬𝐚𝐫𝐢𝐚𝐥 𝐀𝐭𝐭𝐚𝐜𝐤? Adversarial attacks involve manipulating input data to deceive machine learning models, often revealing vulnerabilities. These attacks can significantly degrade model performance, as seen in my project.
 
🧠 𝐖𝐡𝐚𝐭 𝐢𝐬 𝐭𝐡𝐞 𝐕𝐚𝐫𝐢𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐀𝐮𝐭𝐨𝐞𝐧𝐜𝐨𝐝𝐞𝐫 𝐓𝐞𝐜𝐡𝐧𝐢𝐪𝐮𝐞? The variational autoencoder (VAE) is a generative model that learns to encode input data into a latent space and then decodes it back to reconstruct the original data. VAEs are powerful for generating new, similar data, which can be used to create perturbed inputs for testing model robustness.
 
🔮 𝐅𝐮𝐭𝐮𝐫𝐞 𝐒𝐜𝐨𝐩𝐞: This project opens up several exciting avenues for future research and development, such as enhancing the robustness of machine learning models against adversarial attacks, exploring advanced techniques to detect and counter adversarial perturbations, and collaborating on interdisciplinary projects to apply these findings in real-world scenarios, such as cybersecurity and autonomous systems.
 

